{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f09ff6",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../Images/datasafari-logo-primary.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Session 3**\n",
    "\n",
    "# **Working with Data in Python (NumPy & Pandas)**\n",
    "\n",
    "**About the session:** This session is general intoduction of NumPy and Pandas for data handling, including importing and structuring data, inspecting DataFrames, feature selection, and basic cleaning and manipulation.\n",
    "\n",
    "## üéØ Session Objectives\n",
    "<details>\n",
    " <summary> By the end of this session, learner should be able to:</summary>\n",
    " \n",
    "- Create and manipulate NumPy arrays\n",
    "- Build Pandas Series and DataFrames from Python data structures\n",
    "- Load data from CSV/Excel files\n",
    "- Inspect datasets (shape, types, unique values)\n",
    "- Handle missing values (drop, fill)\n",
    "- Remove duplicates and clean data\n",
    "- Prepare datasets for analysis\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **üåæ NumPy Arrays & Operations (Estimated time: 30 minutes)**\n",
    "\n",
    "**Scenario:** A Tanzanian agronomist collects soil moisture readings from a 3x3 grid of sensors across a field. Each row of sensors has readings over time, forming a 3√ó3 \"\"array\"\". How can we store and manipulate this data efficiently?\n",
    "\n",
    "<details> \n",
    "<summary> \"\"Array\"\" üòê! what is it? </summary>\n",
    "\n",
    "**A NumPy array** \n",
    "- Is a special container for numbers (or data) that makes calculations and data analysis easy, efficient perfect for data work!\n",
    "- in other words, An **array** in NumPy is like a super-powered list that can store lots of numbers (or other data) in a neat, organized way like a table or a grid.\n",
    "\n",
    "**ndarray**\n",
    "An ndarray (short for \"n-dimensional array\") in NumPy is a special container that holds numbers (or data) in a structured way like a list, table, or even a stack of tables.\n",
    "- If you think of a list of numbers, that's a 1D ndarray.\n",
    "- If you think of a table with rows and columns (like Excel), that's a 2D ndarray.\n",
    "- If you imagine a stack of tables, that's a 3D ndarray.\n",
    "- It lets you store and work with lots of data efficiently, making calculations and analysis much faster and easier than using regular Python lists.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Why use arrays? </summary>\n",
    "- They let you do math on all the numbers at once (like adding, multiplying, or finding the average), which is much faster than working with regular Python lists.\n",
    "- Arrays help you organize and analyze data, such as rainfall in different regions or temperatures over time.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56244bfb",
   "metadata": {},
   "source": [
    "**Example**: Create a 1D array of 12 monthly rainfall values and then reshape it into a 3√ó4 grid (e.g., 3 regions, 4 months):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf669be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D array representing monthly rainfall data (in mm\n",
    "rainfall_1d = np.array([120, 90, 85, 100, 150, 130, 160, 140, 110, 95, 80, 105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original array (1D array)\n",
    "print(\"Original shape:\", rainfall_1d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fef7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to 3 rows and 4 columns\n",
    "rainfall_grid1 = rainfall_1d.reshape(3, 4)  # 3 rows, 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the reshaped array\n",
    "print(\"Reshaped (3√ó4):\\n\", rainfall_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543339d5",
   "metadata": {},
   "source": [
    "*Try to reshape into **4x4 dimensional array** and observe the results*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to reshape into 4 rows and 4 columns\n",
    "rainfall_grid2 = rainfall_1d... # 4 rows, 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> What happen after attemping to reshape \"rainfall_1d\" array into 4x4 dimensinal array? </summary>\n",
    "\n",
    "- A valid reshape must preserve the total number of elements (here 12 stays 12). If you tried `rainfall_1d.reshape(4, 4)`, NumPy would raise an error because 16‚â†12.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Numpy has built in functions that can be used to perform different data munipulation tasks. Common NumPy functions for data synthesis and manipulation include:</summary>\n",
    "\n",
    "- `np.array()`‚Äì Create arrays from lists or other data.\n",
    "- `np.arange()` ‚Äì Create arrays with regularly spaced values (like Python‚Äôs range).\n",
    "- `np.linspace()` ‚Äì Create arrays with a specified number of evenly spaced values between two numbers.\n",
    "- `np.zeros()` / `np.ones()` ‚Äì Create arrays filled with zeros or ones.\n",
    "- `np.random.rand()` / `np.random.randn()` ‚Äì Generate arrays with random values (uniform or normal distribution).\n",
    "- `np.reshape()` ‚Äì Change the shape of an array without changing its data.\n",
    "- `np.concatenate()` / `np.vstack()` / `np.hstack()` ‚Äì Combine arrays together.\n",
    "- `np.sum()`, `np.mean()`, `np.min()`, `np.max()`, `np.std()` ‚Äì Calculate summary statistics.\n",
    "- `np.sort()` ‚Äì Sort array elements.\n",
    "- `np.unique()` ‚Äì Find unique values in an array.\n",
    "- `np.where()` ‚Äì Find indices or filter elements based on a condition.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> How to breack down the array elements and access certain data point in an array? is slicing and indexing applicable in Numpy?</summary>\n",
    "\n",
    "NumPy arrays support slicing and indexing like Python lists, but in multiple dimensions. For instance, `rainfall_grid1[0, 2]` gives the value in row 0, column 2. You can also slice ranges (e.g., `rainfall_grid1[1:, :]` to get all of rows 1 onward).\n",
    "</details>\n",
    "\n",
    "*Try slicing and indexing on `rainfall_grid1` Numpy array in the code cell bellow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cfacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the value in row 0, column 2\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all of rows 1 onward\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb5c5c",
   "metadata": {},
   "source": [
    " ***Practical task: Create arrays of different shapes and practice indexing and basic operations.***\n",
    " - Use different Numpy built in function to create Numpy arrays and perform besic operations\n",
    " - Follow the guide in the comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D array with 12 elements\n",
    "# Use np.arange to generate values from 0 to 11\n",
    "array1d = np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1168b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape 1D array to 2D array\n",
    "array2d = array1d.reshape(3, 4)\n",
    "\n",
    "# Print the reshaped 2D array\n",
    "print(\"2D array:\\n\", ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5002c83",
   "metadata": {},
   "source": [
    "***3D array example***: \n",
    "- Suppose you have 2 measurements (e.g. temperature and humidity) for each of 4 villages over 3 days.\n",
    "- Use Numpy array to create and analyse this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D array with 24 elements\n",
    "# Use np.arange to generate values from 0 to 23\n",
    "array1d = np.arange(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a66d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape 1D array to 3D array with shape (2, 3, 4)\n",
    "array3d = array1d.reshape(...) # shape: (2 measurement types, 3 days, 4 villages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e64b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the reshaped 3D array shape\n",
    "print(\"3D array shape:\", array3d....) # shape: (2 measurement types, 3 days, 4 villages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb638215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing example: get humidity (index 1) on day 2 (index 1) across all villages\n",
    "humidity_day2 = array3d[1, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the humidity on day 2 across all villages\n",
    "print(\"Humidity on day 2 across villages:\", humidity_day2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Activity:** (15 minutes) On your own machine, create a NumPy array to represent crop yields (in tons) from 5 farms over 6 months. Experiment with reshaping it into different dimensions (e.g., 5√ó6 or 2√ó15) and indexing specific entries (like yield for farm 3 in month 4). Verify that total sum of elements remains the same after reshaping.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìö Pandas Series & DataFrame (Estimated time: 30 minutes)**\n",
    "\n",
    "**Scenario:** A researcher has collected data on student enrollment and performance from several Tanzanian schools. Each row of data includes a student‚Äôs ID, math score, reading score, and whether they attend primary or secondary school. How can we represent and manipulate this tabular data?\n",
    "\n",
    "- *Can any one recall and explain how is the data represented in **Excel sheet**?*\n",
    "<details>\n",
    "<summary> Panda like Excel in Python? can you imagin üòê! </summary>\n",
    "\n",
    "**Pandas** is a Python library that makes it easy to work with data, especially tables (like in spreadsheets/Excel or databases). \n",
    "- It helps to organize, analyze, and clean data using simple commands. \n",
    "- With Pandas, you can quickly load data, look at it, fix missing values, and do calculations all in a way that‚Äôs easy to read and understand.\n",
    "</details>\n",
    "<details>\n",
    "<summary> Pandas introduces two core structures: \"Series\" and \"DataFrame\"</summary>\n",
    "\n",
    "* **Series:** A one-dimensional labeled array (like a column in a table).\n",
    "\n",
    "* **DataFrame:** A two-dimensional table with labeled rows and columns (like in spreadsheet or Excel).\n",
    "</details>\n",
    "<details>\n",
    "<summary> What is the Key difference between the two structure? </summary>\n",
    "\n",
    "A **Series** is essentially a single column of data with an index. A **DataFrame** is a collection of Series sharing the same index, forming rows and columns. \n",
    "- For example, the math scores column of student data would be a Pandas Series. Each column (age, score, etc.) is a Series; \n",
    "- The DataFrame itself has both row (student) and column (feature) dimensions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b06faa",
   "metadata": {},
   "source": [
    "Working with Pandas Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts you need to install pandas library if not already installed\n",
    "# !pip install pandas\n",
    "\n",
    "# Import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d926fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a dictionary\n",
    "# \n",
    "data = {\n",
    "    'StudentID': [101, 102, 103, 104],\n",
    "    'MathScore': [80, 72, 90, 88],\n",
    "    'ReadingScore': [85, 78, 95, 90],\n",
    "    'SchoolType': ['Primary', 'Primary', 'Secondary', 'Secondary']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"data\" dictionary into a DataFrame\n",
    "df_students = pd.DataFrame(...)\n",
    "\n",
    "# Print the DataFrame to see the tabular data\n",
    "print(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62c7ef",
   "metadata": {},
   "source": [
    "- The code above yields a DataFrame with 4 rows and 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of the 'MathScore' column\n",
    "print(type(df_students['MathScore']))  # column is a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea53950f",
   "metadata": {},
   "source": [
    "- Notice that `df_students['MathScore']` is a Pandas Series (1D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Build DataFrames from lists/dicts and check types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Series from a list (e.g., monthly rainfall)\n",
    "rain_series = pd.Series([300, 250, 400, 500], index=['Jan', 'Feb', 'Mar', 'Apr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data in the Series\n",
    "print(\"Rainfall Series:\\n\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba959e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the type() function to check the type of the Series \n",
    "print(\"Type:\", type(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a06ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from dict (education data example)\n",
    "edu_data = {\n",
    "    'School': ['S1', 'S1', 'S2', 'S2'],\n",
    "    'Grade': [5, 5, 6, 6],\n",
    "    'AvgScore': [78, 82, 75, 80]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9282a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary into a DataFrame\n",
    "df_edu = pd....(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b1282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame \n",
    "print(\"\\nEducation DataFrame:\\n\", ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the DataFrame\n",
    "print(\"DataFrame shape:\", ...) # (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Activity:** (15 minutes) \n",
    "- Create a Pandas DataFrame representing an environmental dataset (for example, tree species and count in several Tanzanian forests). \n",
    "- Use a list or dictionary to construct it. Then extract one column as a Series, and verify that it has one dimension. Try `df.info()` to check data types and non-null counts.\n",
    "\n",
    "---\n",
    "\n",
    "## **üì• Loading Data (CSV/Excel) with Pandas (Estimated time: 20 minutes)**\n",
    "\n",
    "**Scenario:** You have received a CSV file from a field survey: it contains crop type, region, and harvest weight for various Tanzanian farms. Now you need to load the file in Python to work with the data.\n",
    "\n",
    "<details>\n",
    "<summary> How to load this file into Pandas for analysis? </summary>\n",
    "\n",
    "**Pandas** makes it easy to load external data. For CSV files, use `pd.read_csv('filename.csv')`; for Excel files, use `pd.read_excel('filename.xlsx')`. These functions return a DataFrame containing the file‚Äôs data. For large datasets, you can also specify parameters (e.g., `usecols`, `dtype`), but basic usage is straightforward:\n",
    "</details>\n",
    "\n",
    "**Example** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file \n",
    "try:\n",
    "    df_soil = pd.read_csv('FILE_PATH_TO_CSV')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please ensure 'soil_quality.csv' is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can similarly load Excel spreadsheets (useful when government data is shared as .xlsx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Excel loading (pandas needs openpyxl or xlrd installed)\n",
    "try:\n",
    "    df_school = pd.read_excel('../Data/tanzania_school_data.xlsx', sheet_name='Sheet1')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please ensure 'tanzania_school_data.xlsx' is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be mindful of file paths: use quotes around the filename and include the correct extension.\n",
    "\n",
    "**Practical Activity:** (10 minutes) Imagine you have a dataset *Financial Inclusion in Tanzania* in CSV file. \n",
    "- Write code to load it using `pd.read_csv`, \n",
    "- Then display the first few rows with `.head()`. \n",
    "- Confirm that the loaded file contain the targeted datasets by checking the columns names.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Exploring and Summarizing Data (Estimated time: 30 minutes)\n",
    "\n",
    "**Scenario:** After loading the Financial Inclusion data, you need to understand the dataset‚Äôs structure. What are the dimensions? What types of data are present? Are there any strange values or typos?\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary> How to check the general structure and composition of the loaded data?</summary>\n",
    "\n",
    "- Check the shape of the DataFrame: `df.shape` gives (rows, columns). \n",
    "- The `df.info()` method lists each column with its data type and count of non-null entries, which helps spot missing values. For example, if one column has fewer non-null than another, it hints at missing weights.\n",
    "- `df.describe()` gives summary statistics for numeric columns (mean, std, min/max, quartiles) and for object (string) columns it shows count, unique, most frequent, etc. This quickly highlights any unexpected values and the spread of data. \n",
    "- Check unique values or categories: for a categorical column like ‚Äúregion‚Äù or ‚ÄúSchoolType‚Äù, `df['region'].value_counts()` or `.nunique()` shows how many distinct categories and their frequencies. Value counts help confirm if encoding is consistent \n",
    "</details>\n",
    "\n",
    "**Example exploration:** Simulated ***Soil Quality*** data.\n",
    "+ Load the `soil_quality` data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "soil_quality = ... # pd.read_csv('../Data/soil_quality.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69623a3b",
   "metadata": {},
   "source": [
    "+ Check the first 5 rows and column names to confirm the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb146bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .head() to check the first 5 rows and column names\n",
    "soil_quality... # .head(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Check dimensions, data types, and key statstics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dimensions \n",
    "print(\"\\nShape:\")\n",
    "....shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data information (data types, non-null counts)\n",
    "print(\"\\nData information:\")\n",
    "soil_quality... # .info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key statistics for numeric columns\n",
    "print(\"\\nDescribe numeric:\\n\")\n",
    "soil_quality... # .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Unique values in 'Location' column\n",
    "print(\"\\nUnique locations:\")\n",
    "...['Location']... # .unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5112c1",
   "metadata": {},
   "source": [
    "+ Check how many observation was made at each location (station)\n",
    "  - use `.value_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95aa773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts at each location\n",
    "print(\"Observation at each lake:\\n\")\n",
    "...['Location']... # .value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Activity:** (15 minutes) Load the *Financial Inclusion data*. \n",
    "- Use the Pandas methods (`.shape`, `.info()`, and `.describe()`) to inspect the data\n",
    "- Look and identify important features that need further investigations e.g., Gender, education level, bank ownership status, etc. \n",
    "- Use `.value_counts()` to note the number of records, identify numeric vs text columns, spot any obvious anomalies, and see how many unique categories exist in a categorical column.\n",
    "---\n",
    "- ***Task:*** The **Financial Inclusion** data was used in ***AI-Wizards***, recall the important feature that were identified and discused to be important for ML model development with **Orange Data Mining**. Prepare proper names for those features as will be used in the next sections of this session.\n",
    "\n",
    "----\n",
    "----\n",
    "DATA CLEANING AND PREPROCESSING\n",
    "----\n",
    "\n",
    "## üéØ Feature Selection (Indexing & Slicing) (Estimated time: 20 minutes)\n",
    "\n",
    "**Scenario:** Now the *Finatial inclusion data* have been loaded and inspected succefully, as observed the data has a lot of features(Columns) of which not all are needed. Suppose you want to work with and analyze only the few information contained in a specific column.. How can you subset the DataFrame to these rows/columns? Here is where the concept of feature selection comes in, and this is where real life use of slicing and indexing concept applys.\n",
    "\n",
    "<details>\n",
    "<summary> Subsetting: How Indexing can return Pandas-series or Pandas-DataFrame?</Summary>\n",
    "\n",
    "Pandas makes subsetting easy via indexing. To select specific columns, use `df['col']` for a single column (returns a Series) or `df[['col1', 'col2']]` for multiple columns (returns a DataFrame). For example you have Student data loaded in Pandas data as `df_students`. The data has 'StudentID', 'MathScore', 'AvgScore', 'SchoolType' and 'Grade' columns and want to filter out certain columns. \n",
    "- `df_students['MathScore']` extracts just the math scores and return  1D array (Series)\n",
    "- `df_students[['MathScore', 'Grade']]` extract math score and grade and return 2D array (DataFrame)\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> How to select columns that meet specific criteria/conditions?</summary>\n",
    "\n",
    "In this case a **boolean indexing** is used. Forexample\n",
    "- To select only Primary school data, `df_students[df_students['SchoolType']=='Primary']` returns all rows where SchoolType is ‚ÄúPrimary‚Äù (all primary school students). \n",
    "- To select only schools and average score of students with grade exactly equal 5, then `df_students[['School','AvgScore']][df_students['Grade'] == 5]` selects grade 5 from columns School and AvgScore.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary> Slicing rows and column by position and name: .loc and .iloc!</summary>\n",
    "\n",
    "Pandas also supports `.loc` and `.iloc` for label-based and integer-based indexing.\n",
    "Use .loc for label-based indexing and .iloc for position-based indexing in Pandas DataFrames and Series.\n",
    "- `.loc`: Selects rows and columns by their labels (names).\n",
    "   - Example: `df_students.loc[2, 'MathScore']` selects the value in row with index label 2 and column 'MathScore'. Use when you know the row/column labels.\n",
    "- `.iloc`: Selects rows and columns by their integer position (like Python lists).\n",
    "   - Example: `df_students.iloc[2, 1]` selects the value at the third row and second column (0-based index). Use when you want to select by position, not label.\n",
    "</details>\n",
    "\n",
    "**Example:** From the loaded `soil_quality` data perform the following:-\n",
    "\n",
    "- Select only one column the `pH` column\n",
    "- Explain what is dtype of the resulting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c95037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'pH' column as a Series\n",
    "ph_data = soil_quality..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9980e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the pH data\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e718b",
   "metadata": {},
   "source": [
    "- Select multiple columns: Filter only `Location` and `pH`\n",
    "- What is the dtype of resulting output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'Location' and 'pH' columns\n",
    "ph_location = soil_quality..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90257db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the pH data and its type\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216d136",
   "metadata": {},
   "source": [
    "- Filtering rows by condition (**Boolean indexing**): from the pH scale, select only the Acidic soils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189195bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows where pH < 7 (acidic soils)\n",
    "acidic_soils = ... #soil_quality[soil_quality['pH'] < 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the acidic soils\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204f07d",
   "metadata": {},
   "source": [
    "- Slicing rows and column by position (`.iloc`): Select the First 5 rows of 'Location' and 'pH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 5 rows using .iloc (position-based)\n",
    "... # first_five_rows = soil_quality.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the resulting rows\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 5 rows of 'Location' and 'pH' columns using .iloc (position-based)\n",
    "soil_quality.iloc[...:..., [..., ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0803e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the output\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73028b12",
   "metadata": {},
   "source": [
    "- Slicing rows and column by names (`.loc`): Select the First 5 rows of 'Location' and 'pH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c64c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 5 rows using .loc (label-based)\n",
    "first_five_rows_loc = soil_quality... # .loc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the output\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1638fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 5 rows of 'Location' and 'pH' columns using .loc (label-based)\n",
    "soil_quality.loc[:..., [..., ...]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982d529",
   "metadata": {},
   "source": [
    "- Discuss the Key difference between the two methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical Activity:** (10 minutes) *Financial inclusion data feature selection and preparation*\n",
    "##### ***What features need to be selected?***\n",
    "<details>\n",
    "<summary>Problem description: Why is it important to first identify and describe the data science problem in relation to the available datasets?</summary>\n",
    "\n",
    "It is important to first identify and describe the data science problem in relation to the available datasets because:\n",
    "- It ensures understand the goal of analysis and what questions needs to be answered.\n",
    "- It allows to check if the dataset contains the necessary features and enough quality data to solve the problem.\n",
    "- It helps **selecting relevant features and data needed for the problem, avoiding unnecessary or irrelevant information**.\n",
    "- It guides data cleaning, transformation, and modeling steps to be focused and efficient.\n",
    "- It makes communication with stakeholders clearer, as everyone knows what the analysis aims to achieve.\n",
    "</details>\n",
    "<details>\n",
    "<summary>What problem needs to be solved/answere from Tanzania financial inclusion scenario?</summary>\n",
    "\n",
    "- Imagin if you had a model that could predict or classify whether an individual **does** or **does not** have a **bank account**, what would be its application and for whom?\n",
    "- In this case the task is to develop a ML model that can help stakeholders to classify individual with and without bank account based different factors\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>What are some differentiating factors that could help classify individuals between having and not having a bank account?</summary>\n",
    "\n",
    "- The FinScope Tanzania 2023 dataset has too many (721!) variables to analyze together so we need to choose features and also rename them as they are named using codes.\n",
    "- Here is where **feature selection** comes in.\n",
    "- These features can be grouped into two categories, the *Financial factors* and *General demographic factors*.\n",
    "- For the purpose of this session the features in the table bellow will be selected and used.\n",
    "\n",
    "|Financial factors|General demographic factors|\n",
    "|-----------------|---------------------------|\n",
    "|Employment/salary/pension status|*Age - `c8c`|\n",
    "|Income source/level - `IncomeMain`|*Gender - `c9`|\n",
    "|Savings habits - `e_7_n_3`|*Martial status - `c10`|\n",
    "|Loan borrowing status - `g_2_1_1`|*Educational level - `c11`|\n",
    "|Financial education - `e_5_2`|*Geographic location - `RU`|\n",
    "|          |Identification (NIDA) number - `c27__2`|\n",
    "|          |Smartphone ownership - `c25__1`|\n",
    "-  Unlike in **Orange Data-mining** tool, in python the target feature `'comm3_1'` which is 'Bank account ownership status' will also be included here for preprocessing, will be excluded later before Traing and test split.\n",
    "\n",
    "*NOTE:* For detailed explanation of finding the codes of these feature refers to *Session 2* of\n",
    "[AI Wizards](https://docs.google.com/presentation/d/1ut-eSE56S620Z2JNZ5XzPum8xPF8n6bcarS6nuw1paE/edit?usp=sharing)\n",
    "</details>\n",
    "\n",
    "##### ***Selecting and renaming features***\n",
    "- Make sure Pandas is imported and Load the `FinScope.csv` file if not loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8d4ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas if not already imported\n",
    "import pandas as pd\n",
    "\n",
    "# Load the `FinScope.csv` file if not loaded\n",
    "df_finscope = pd.read_csv('../Data/FinScope.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1080217d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SN       reg_name  reg_code  dist_code dist_name  ward_code1 ward_name  \\\n",
      "0  4529         Mwanza        19          7  Misungwi         251     Mondo   \n",
      "1  4245         Kagera        18          7  Missenyi          11   Kakunyu   \n",
      "2  8149          Mbeya        12          3     Kyela         283     Nkuyu   \n",
      "3  6763         Dodoma         1          3    Kongwa         123  Kibaigwa   \n",
      "4  7805  Dar es Salaam         7          2     Ilala         252    Majohe   \n",
      "\n",
      "   ea_code clustertype        c1  ...      SOCIAL_GROUPS  OTHER_FORMAL  \\\n",
      "0        4       Rural  Original  ...  Not SOCIAL_GROUPS  OTHER_FORMAL   \n",
      "1        1       Rural  Original  ...  Not SOCIAL_GROUPS  OTHER_FORMAL   \n",
      "2      301       Urban  Original  ...  Not SOCIAL_GROUPS  OTHER_FORMAL   \n",
      "3      301       Urban  Original  ...  Not SOCIAL_GROUPS  OTHER_FORMAL   \n",
      "4       29       Urban  Original  ...  Not SOCIAL_GROUPS  OTHER_FORMAL   \n",
      "\n",
      "   OVERALL_FORMAL                                        INFORMAL  \\\n",
      "0  OVERALL_FORMAL  INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS   \n",
      "1  OVERALL_FORMAL  INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS   \n",
      "2  OVERALL_FORMAL  INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS   \n",
      "3  OVERALL_FORMAL  INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS   \n",
      "4  OVERALL_FORMAL  INFORMAL incl SACCO AND CMG RISK CONTRIBUTIONS   \n",
      "\n",
      "                    fasx             overlapx                  IncomeMain  \\\n",
      "0  Other formal non-bank  informal AND formal         Farmers and fishers   \n",
      "1  Other formal non-bank  informal AND formal         Farmers and fishers   \n",
      "2  Other formal non-bank  informal AND formal     Piece work/casual labor   \n",
      "3  Other formal non-bank  informal AND formal  Traders - non-agricultural   \n",
      "4  Other formal non-bank  informal AND formal         Farmers and fishers   \n",
      "\n",
      "              BusO      DEDICATED_FARMER      SMALLHOLDER_FARMER  \n",
      "0              nor  Not Dedicated farmer  Not Smallholder farmer  \n",
      "1              nor  Not Dedicated farmer  Not Smallholder farmer  \n",
      "2              nor  Not Dedicated farmer  Not Smallholder farmer  \n",
      "3  Business owners  Not Dedicated farmer  Not Smallholder farmer  \n",
      "4              nor  Not Dedicated farmer  Not Smallholder farmer  \n",
      "\n",
      "[5 rows x 721 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the loaded file to confirm data\n",
    "print(df_finscope.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deafff2",
   "metadata": {},
   "source": [
    "- Prepare a list of **column names** i.e **variable codes** as shown in the Table above\n",
    "`columns_names = 'IncomeMain', 'e_7_n_3', 'g_2_1_1', 'e_5_2', 'c8c', 'c9', 'c10', 'c11', 'RU', 'c27__2', 'c25__1', 'comm3_1'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48fe47f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature codes to select\n",
    "column_names = ['IncomeMain', 'e_7_n_3', 'g_2_1_1', 'e_5_2', 'c8c', 'c9', 'c10', 'c11', 'RU', 'c27__2', 'c25__1', 'comm3_1'] # list of feature codes as shown in the Table above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54775c6e",
   "metadata": {},
   "source": [
    "- Use the created list of `column_names` to extract the data from the loaded data\n",
    "- This is multiple column indexing for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f4ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns using the list of column names\n",
    "df_selected = df_finscope[column_names]\n",
    " # Pass the list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081befb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   IncomeMain e_7_n_3 g_2_1_1  e_5_2  c8c      c9  \\\n",
      "0         Farmers and fishers      No     Yes  False   47  Female   \n",
      "1         Farmers and fishers     Yes      No  False   63  Female   \n",
      "2     Piece work/casual labor     Yes     Yes   True   74    Male   \n",
      "3  Traders - non-agricultural     Yes     Yes  False   29  Female   \n",
      "4         Farmers and fishers     Yes     Yes  False   53    Male   \n",
      "\n",
      "                       c10                  c11             RU c27__2 c25__1  \\\n",
      "0  Married/living together         Some primary          Rural     No     No   \n",
      "1                  Widowed  No formal education          Rural     No          \n",
      "2                  Widowed         Some primary    Other urban    Yes     No   \n",
      "3       Divorced/separated         Some primary    Other urban     No     No   \n",
      "4  Married/living together    Primary completed  Dar es Salaam    Yes     No   \n",
      "\n",
      "  comm3_1  \n",
      "0          \n",
      "1          \n",
      "2          \n",
      "3          \n",
      "4          \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the selected data and comfirm the selection\n",
    "print(df_selected.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1ef06",
   "metadata": {},
   "source": [
    "- The coded names are irrelevant and probably difficult to remember or refer\n",
    "- Rename the coded column names to any name that is relevant to you. You can use modified names in the table below\n",
    "<details>\n",
    "<summary>Table: Reference names</summary>\n",
    "\n",
    "|Financial factors|General demographic factors|\n",
    "|-----------------|---------------------------|\n",
    "|Employment/salary/pension status|`c8c` => `Age`|\n",
    "|`IncomeMain` => `Income_source`|`c9` => `Gender`|\n",
    "|`e_7_n_3` => `Savings_habits`|`c10` => `Martial_status`|\n",
    "|`g_2_1_1` => `Borrowing_status`|`c11` => `Educational_level`|\n",
    "|`e_5_2` => `Financial_education`|`RU` => `Geographical_location`|\n",
    "|`comm3_1` => `has_bank_account`|`c27__2` => `NIDA_number`|\n",
    "|          |`c25__1` => `Smartphone_ownership`|\n",
    "\n",
    "</details>\n",
    "\n",
    "- Here use `.rename()` method and pass in a dictionary of old names as Key and new names as Values as shown in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf4b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_renamed = df_selected.rename(columns={\n",
    "    'IncomeMain': 'Income_source',\n",
    "    'e_7_n_3': 'Savings_habits',\n",
    "    'g_2_1_1': 'Borrowing_status',\n",
    "    'e_5_2': 'Financial_education',\n",
    "    'c8c': 'Age',\n",
    "    'c9': 'Gender',\n",
    "    'c10': 'Marital_status',\n",
    "    'c11': 'Educational_level',\n",
    "    'RU': 'Geographic_location',\n",
    "    'c27__2': 'NIDA_number',\n",
    "    'c25__1': 'Smartphone_ownership',\n",
    "    'comm3_1': 'has_bank_account'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c545bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows and confirm the renaming\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5f799",
   "metadata": {},
   "source": [
    "Now investigate the filtered data and selected feature\n",
    "- Use the Pandas methods (`.shape`, `.info()`, and `.describe(include = 'all')`) to inspect the data\n",
    "   <details>\n",
    "   <summary>What is the difference of .describe() and .describe(include = 'all) ?</summary>\n",
    "\n",
    "   `.describe(include='all')` in Pandas returns summary statistics for all columns in a DataFrame, including both numeric and non-numeric (object, categorical, boolean) types.\n",
    "   - It shows count, unique values, top (most frequent) value, frequency, mean, std, min, max, and quartiles where applicable.\n",
    "   - This helps you quickly understand the distribution and characteristics of every column, not just numeric ones.\n",
    "   </details>\n",
    " - Investigate carefull the summary statistics of all features\n",
    "\n",
    "- Use `.value_counts()` to note the number of records, identify numeric vs text columns, spot any obvious anomalies, and see how many unique categories exist in a categorical column.\n",
    "  - Then run the `.value_counts()` only on target feature and ***note the counts***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3688a345",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63d14b",
   "metadata": {},
   "source": [
    "## ‚ùì Detecting Missing Data (Estimated time: 15 minutes)\n",
    "\n",
    "**Scenario:** Real-world data often has missing entries. In a survey, some farmers may have skipped reporting their fertilizer use (NaN values). \n",
    "\n",
    "<details>\n",
    "<summary>How do we detect these gaps?</summary>\n",
    "\n",
    "Pandas uses `NaN` (and `pd.NA`) to represent missing values. To detect them, use `df.isna()` or `df.isnull()`, which return a boolean DataFrame of the same shape indicating `True` for missing entries. For example, `df[['Nitrogen','pH']].isna().sum()` tells you how many missing in those columns. The DataFrame method `.isna()` flags any `None` or `np.nan` as `True`.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Example:** Lets look on the `soil_quality` data\n",
    "- Load and display the last fow columns of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of soil_quality DataFrame\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb362d5",
   "metadata": {},
   "source": [
    "- Use `.isna()` to look for Missing values\n",
    "- Can you count how many missing and in which column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9604d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .isna() to detect missing values\n",
    "soil_quality... # .isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4c566",
   "metadata": {},
   "source": [
    "- For data with few entries and fow column is possible to count\n",
    "- For many cases and many column manual counting is not practical\n",
    "- Then use `.isna().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "... # .isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67c800",
   "metadata": {},
   "source": [
    "This step is crucial before cleaning. Knowing which columns have missing data (and how many) guides whether to drop or fill values.\n",
    "\n",
    "**Practical Activity:** (10 minutes) \n",
    "Now from the *Financial inclussion scenario* \n",
    "- You ended on selecting a subset of variables and asssigned to `df_renamed`, run `.isna()` and `.isna().sum()` to see which columns contain missing data. \n",
    "- Identify if any critical column has missing entries that need handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Missing Values\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36e3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3df845d",
   "metadata": {},
   "source": [
    "- From the summary abover it shows that all the selected variables has NO missing values\n",
    "- Run value counts on the `has_bank_account`, again very carefull observation on the counts and **note the difference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ce882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts for 'has_bank_account' column\n",
    "df_renamed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d63a7c",
   "metadata": {},
   "source": [
    "\n",
    "- What do you observe?\n",
    "   - Is it true that the column contain all the data as needed?\n",
    "   <details>\n",
    "   <summary>What does this mean?</summary>\n",
    "\n",
    "   The first line (8559) corresponds to blank strings (\"\"), not actual NaN values. That‚Äôs why:\n",
    "   - `df_renamed.isna().sum()` shows 0 missing values (because technically nothing is NaN).\n",
    "   - But logically, those empty strings should be treated as missing values.\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary>What to do in this situation?</summary>\n",
    "\n",
    "   - Convert empty strings to NaN so Pandas recognizes them as missing\n",
    "   - Then `.isna().sum()` will correctly count these as missing\n",
    "   </details>\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy library\n",
    "import numpy as np\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22c986",
   "metadata": {},
   "source": [
    "- Replace empty strings with NaN in columns\n",
    "- this will fix the issue of only the white space or empyt characters in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chech how many empyty space strings are in the column\n",
    "df_renamed['has_bank_account'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty strings (' ') with NaN in 'has_bank_account' column\n",
    "df_replaced = ...replace(' ', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e70e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values again\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d939dc6",
   "metadata": {},
   "source": [
    "This is just one column in the data and only one problem of empyt space. The data contain multiple columns of Object/string, in each of these column probably also contain empyt spaces, special characters, numbers etc, which are not needed.\n",
    "<details>\n",
    "<summary>What might be implication of this problem and how to solve it?</summary>\n",
    "\n",
    "- Special characters often sneak into categorical columns when the dataset was exported/imported (CSV/Excel), or during encoding/renaming. They can cause issues because for example *\"One account\"*, *\"One account \"*, and *\"One account\"* (with a non-breaking space) all look the same but are actually different strings.\n",
    "- Sometimes the string column might contain mixed of lower and UPPER case characters\n",
    "- These are just a few to mention, but there are might be other many issues associated with string/object data\n",
    "- **Dealing with** these issue one by one might be not practical as it require a lot of time and energ. Here is where **Function** comes in.\n",
    "- Function can automate the process of solving the issue.\n",
    "\n",
    "</details>\n",
    "\n",
    "Lets build reusable **cleaning_function** that can be apply to the entire DataFrame (or just one column). The function handles:\n",
    "- Converting values to strings\n",
    "- Stripping whitespace (leading/trailing)\n",
    "- Removing invisible/special characters\n",
    "- Replacing empty strings with NaN\n",
    "- Standardize case (e.g., make everything lowercase so \"One account\" and \"ONE ACCOUNT\" are treated the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text columns in a DataFrame\n",
    "def clean_text_columns(df, columns=None, case=\"lower\"):\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=['object', 'string']).columns\n",
    "    \n",
    "    for col in columns:\n",
    "        # convert to string and strip spaces\n",
    "        series = df[col].astype(str).str.strip()\n",
    "        \n",
    "        # remove special characters\n",
    "        series = series.str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "        \n",
    "        # handle case standardization\n",
    "        if case == \"lower\":\n",
    "            series = series.str.lower()\n",
    "        elif case == \"upper\":\n",
    "            series = series.str.upper()\n",
    "        elif case == \"title\":\n",
    "            series = series.str.title()\n",
    "        \n",
    "        # replace empty strings and 'nan' with proper NaN\n",
    "        df[col] = series.replace({'': np.nan, 'nan': np.nan})\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d2a16",
   "metadata": {},
   "source": [
    "- Use the function above to:\n",
    "  - Clean only the `'has_bank_account'` column in `df_renamed`\n",
    "  - Clean all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80902c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean only the 'has_bank_account' column in df_renamed\n",
    "\n",
    "df_cleaned_single = clean_text_columns(..., ...., ...) # Pass in the function parameters as specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "826ea36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all columns in df_renamed\n",
    "\n",
    "df_cleaned_all = clean_text_columns(df_renamed, columns=None, case=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37683ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values again \n",
    "df_cleaned_all.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22e19c",
   "metadata": {},
   "source": [
    "- How many columns now have missing values?\n",
    "- Check the counts of uniqe values in each column with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efed36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392248f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of unique values in 'Smartphone_ownership' column\n",
    "df_cleaned_all[...].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d720443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of unique values in 'has_bank_account' column\n",
    "df_cleaned_all..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2ffa9",
   "metadata": {},
   "source": [
    "- Note the counts of each unique values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Handling Missing Data (dropna & fillna) (Estimated time: 20 minutes)\n",
    "\n",
    "**Scenario:** Continuing with the fertilizer example, suppose several fields have no recorded nitrogen values. We have choices: remove those records or fill them based on other data. \n",
    "\n",
    "<details>\n",
    "<summary>How to drop and/or filling missing vaules?</summary> \n",
    "\n",
    "Pandas provides two common methods:\n",
    "\n",
    "* `dropna()`: remove rows (or columns) with missing values. By default, `df.dropna()` drops any row containing any NaN. You can specify `subset=[...]` to only look at certain columns, or `axis=1` to drop columns.\n",
    "* `fillna()`: replace missing values with a specified value or a method (like forward-fill). For example, `df['Nitrogen'].fillna(3.0)` replaces all NaNs in that column with 3.0.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a clean copy of the DataFrame to prevent modifying the original\n",
    "df_soil_copy = soil_quality.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing data\n",
    "df_cleaned = df_soil_copy.dropna()\n",
    "print(\"After dropna:\\n\", df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f390a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or fill missing Nitrogen with the mean\n",
    "mean_n = df_soil_copy['Nitrogen'].mean()\n",
    "df_soil_copy['Nitrogen_filled'] = df_soil_copy['Nitrogen'].fillna(mean_n)\n",
    "print(\"\\nFillNaN with mean:\\n\", df_soil_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping vs. filling depends on context. If few records are missing, dropping (`dropna()`) might be fine. If many, filling (`fillna()`) can preserve data by imputing values.\n",
    "<details>\n",
    "<summary>What are the technical factors to consider when choosing either Dropping or Filling missing values?</summary>\n",
    "\n",
    "üîπ 1. ***Dropping missing values***: Remove rows (or columns) that contain missing data.\n",
    "\n",
    "When to drop:\n",
    "\n",
    "  - If the proportion of missing values is very high in a row/column (e.g., 70%+ missing in a feature, or a row with almost no info).\n",
    "  - If the dataset is large and losing some rows won‚Äôt harm representativeness.\n",
    "  - If the missingness looks completely random and won‚Äôt bias results.\n",
    "  - If the column isn‚Äôt important for your analysis/model.\n",
    "\n",
    "Downsides:\n",
    "\n",
    "  - You lose data (reduces sample size).\n",
    "  - If missingness isn‚Äôt random, you may introduce bias.\n",
    "\n",
    "üîπ 2. ***Filling (Imputation)***: Replace missing values with substitutes.\n",
    "\n",
    "When to fill:\n",
    "\n",
    "  - If the feature is important and you don‚Äôt want to lose it.\n",
    "  - If the dataset is small and dropping would throw away too much information.\n",
    "  - If missingness is systematic (e.g., survey skip patterns).\n",
    "\n",
    "Common imputation methods:\n",
    "\n",
    "**Simple Fill**\n",
    "\n",
    "- Numerical: mean, median, mode\n",
    "  e.g., `df['age'] = df['age'].fillna(df['age'].median())`\n",
    "- Categorical: mode (most frequent value), constant like \"Unknown\"\n",
    "  e.g., `df['has_bank_account'] = df['has_bank_account'].fillna(\"Unknown\")`\n",
    "\n",
    "**Advanced Imputation**\n",
    "\n",
    "- KNN imputer (finds similar rows and fills based on neighbors)\n",
    "\n",
    "- Regression imputation (predicts missing value using other features)\n",
    "\n",
    "- Multiple Imputation (statistical sampling of plausible values)\n",
    "\n",
    "**Domain-driven fill**\n",
    "- Example: If has_bank_account is missing, you might decide it means \"No account\" instead of dropping it ‚Äî but only if you‚Äôre sure about the survey design.\n",
    "    <details>\n",
    "    <summary>üëâ Rule of thumb:</summary>\n",
    "\n",
    "    - Small dataset ‚Üí Fill (to keep all data you can)\n",
    "    - Big dataset ‚Üí Drop (if missingness is small and random)\n",
    "    - Critical variable ‚Üí Fill (never drop)\n",
    "    - High-missing column ‚Üí Drop (usually useless)\n",
    "    </details>\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "**Practical Activity:** (15 minutes) In *Financial inclusion casestudy*, after applying the text cleaning function in all column with missing values:\n",
    "\n",
    "* Check how many percent of the data are missing in each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking % of missing values in each column\n",
    "df_cleaned_all.isna().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a01584",
   "metadata": {},
   "source": [
    "- Check for unique values in columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeac5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of unique values in 'Smartphone_ownership' column\n",
    "df_cleaned_all[...].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34289a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the counts of unique values in 'has_bank_acount' column\n",
    "df_cleaned_all..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad99912",
   "metadata": {},
   "source": [
    "- Which method of dealing with missing values should be adopted in this dataset and why?\n",
    "- Apply the selected method to to fill in missing values\n",
    "   <details>\n",
    "   <summary>Hints</summary>\n",
    "   \n",
    "   * Use `df.fillna()` to fill missing values. \n",
    "   * For `has_bank_account` missing values, try filling with a placeholder \"no account\". Show counts of missing values (`.isna().sum()`).\n",
    "   * For `Smartphone_ownership` replace NAN with the model value in that column\n",
    "   </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bba2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the cleaned DataFrame to avoid modifying the original\n",
    "df_cleaned_all_copy = df_cleaned_all.copy()\n",
    "\n",
    "# Fill missing values in 'Smartphone_ownership' with the mode (most frequent value)\n",
    "df_cleaned_all_copy['...'] = df_cleaned_all_copy....fillna(df_cleaned_all_copy[...].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of unique values in 'Smartphone_ownership' column after filling missing values\n",
    "df_cleaned_all_copy....value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075c7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'has_bank_account' with the placeholder ('no account')\n",
    "df_cleaned_all_copy[...] = df_cleaned_all_copy....fillna('no account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of unique values in 'has_bank account' column after filling missing values\n",
    "df_cleaned_all_copy....value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2b924",
   "metadata": {},
   "source": [
    "- Now the Data contain no missing values as well as no special and empty characters. Also all the string values have been converted into lower cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d9e06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üóëÔ∏è Removing Duplicates (Estimated time: 10 minutes)\n",
    "\n",
    "**Scenario:** In collecting survey data, for example in a formers survey, a farmer might accidentally enter data twice. If the dataset contains duplicate entries for the same farm and date, we should clean them out.\n",
    "\n",
    "<details>\n",
    "<summary>How to detect and removing duplicated entries?</summary>\n",
    "\n",
    "***Detecting duplicated rows***\n",
    "  - Use `.duplicated()` this will show duplicated rows in the dataframe\n",
    "  - Use `.duplicated().sum()` to show the number of duplicated rows\n",
    "\n",
    "***Removing duplicated row***\n",
    "Pandas‚Äô `drop_duplicates()` helps here. \n",
    "  - By default `df.drop_duplicates()` removes any duplicate rows, keeping the first occurrence (you can change `keep='last'` or `keep=False` to drop all copies). \n",
    "  - You can also specify a subset of columns to identify duplicates (e.g., `subset=['FarmID','Date']`).\n",
    "</details>\n",
    "\n",
    "For example: Look at the Demo simulated data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame with duplicate rows\n",
    "df_demo = pd.DataFrame({\n",
    "    'FarmID': [1,1,2,2,2],\n",
    "    'Date': ['2021-01-01','2021-01-01','2021-02-01','2021-02-01','2021-03-01'],\n",
    "    'Yield': [100, 100, 150, 150, 200]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e678b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect duplicated rows\n",
    "df_demo... # .duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of duplicated rows\n",
    "print(...) # df_demo.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd94737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows\n",
    "df_demo = df_demo... # .drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This will remove the second identical row for FarmID=1 on 2021-01-01 and similarly for FarmID=2 on 2021-02-01.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8fff8f",
   "metadata": {},
   "source": [
    "**Practical Activity: üîß** (30 minutes) \n",
    "\n",
    "- Check your dataset for duplicated rows using `df_cleaned_all_copy.duplicated()`. \n",
    "- If any are found, decide how to handle them (e.g., keep the first occurrence). \n",
    "  - Apply `df.drop_duplicates(inplace=True)` and confirm duplicates are gone with `df.duplicated().sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1202a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates \n",
    "df_cleaned_all_copy.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa888a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of duplicated rows\n",
    "print(df_cleaned_all_copy...) # .duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows and create a new DataFrame without duplicates\n",
    "df_final = df_cleaned_all_copy... # .drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e44127",
   "metadata": {},
   "source": [
    "- The data contain no duplicates and ready for futher cleaning process\n",
    "- Disply summary statistics of all columns in `df_final` include categorical variables. Use `.describe(include = 'all)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of all columns\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc95d90",
   "metadata": {},
   "source": [
    "- Inspect carefully the summary statistics table. Pay close attention on 'count', 'unique' and 'top' rows of the table for each variable name.\n",
    "- How many **unique values** are in `Income_source`, `Marital_status`, `Education_level`, `has_bank_account` and `Geographic_location` columns?\n",
    "\n",
    "<details>\n",
    "<summary>What to do for the columns with more than two unique values with the same meaning?</summary>\n",
    "\n",
    "- Check these columns one by one to investigate what are those unique value. Note the consistency of the values and determine if there is any element of repititions. For example, 'one account', 'two or three account' and 'more than three account' are different values but for the purpose of this case they mean the same.\n",
    "- There for instead of four unique values in `has_bank_account` column, the values should be maped into two values of 'yes' for having bank acccount and 'no' for not having bank account.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values in categorical columns\n",
    "df_final.select_dtypes(include=['object', 'category']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 'have_bank_account' to investigate what are those unique value\n",
    "df_final['...'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ddf30",
   "metadata": {},
   "source": [
    "- As you can see there are four unique value, three of them means have bank account\n",
    "- Map the value into `yes` and `no`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .replace() to map the values into 'yes' and 'no'\n",
    "\n",
    "df_final['has_bank_account'] = df_final['has_bank_account'].replace({\n",
    "    'no account': 'no',\n",
    "    'one account': 'yes',\n",
    "    'two or three accounts': 'yes',\n",
    "    'more than three accounts': 'yes'\n",
    "})\n",
    "\n",
    "# Count the unique values again to confirm the mapping\n",
    "df_cleaned_all_copy['has_bank_account'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c527c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values again to confirm the mapping\n",
    "df_final['has_bank_account'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0769c4c",
   "metadata": {},
   "source": [
    "- Repeat the procedure for other categorical variable with redundant values, to map them into meaning unique values\n",
    "- This stape is very important for further data process towards exploratory data analysis (EDA) and model development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748c5f9",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## üßë‚Äçüíª Capstone Project: Cleaning & Preparing Census Income Data in Pandas (40 minutes)\n",
    "\n",
    "<details>\n",
    "<summary>üëâ‚ÄúCan you predict whether an individual earns more than $50,000 per year based on their personal and professional attributes?‚Äù</summary>\n",
    "\n",
    "Imagine you have just been hired as a data scientist at a government research agency. Your team is studying how personal and professional characteristics ‚Äî like age, education, type of job, and hours worked per week ‚Äî are related to a person‚Äôs income level.\n",
    "\n",
    "You are given a large dataset from the U.S. Census Bureau. This dataset contains detailed records about thousands of individuals, including their demographic information, work history, and whether they earn more than $50,000 per year or not.\n",
    "\n",
    "Your mission is to prepare this raw dataset for analysis and prediction. Real-world data is never perfect, so before you can build any predictive models, you must:\n",
    "\n",
    "- Investigate the dataset to understand what‚Äôs inside\n",
    "\n",
    "- Fix missing values and messy categories\n",
    "\n",
    "- Remove duplicates and irrelevant information\n",
    "\n",
    "- Convert categorical features into machine-readable form\n",
    "\n",
    "- Select the features most useful for predicting income\n",
    "\n",
    "By the end of this project, you will have transformed messy census data into a clean, structured dataset ready for machine learning. This work will set the stage for answering the big question: *‚ÄúIs it possible to predict whether an individual earns more than $50,000 per year based on their personal and professional attributes?‚Äù*\n",
    "\n",
    "</details>\n",
    "\n",
    "**Dataset**: [Adult Census Income (UCI)](https://archive.ics.uci.edu/dataset/2/adult)\n",
    "\n",
    "**Goal**: Prepare the dataset so that it‚Äôs ready for machine learning to predict whether a person‚Äôs income exceeds $50K/year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245b278",
   "metadata": {},
   "source": [
    "***Part 1: Load & Explore the Data***\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Load the dataset into Pandas with the correct column names.\n",
    "\n",
    "2. Print the shape of the dataset (rows, columns).\n",
    "\n",
    "3. Inspect the first 10 rows.\n",
    "\n",
    "4. Use `.info()` to see column data types.\n",
    "\n",
    "5. Use `.describe(include=\"all\")` to explore summary statistics.\n",
    "\n",
    "Questions\n",
    "\n",
    " - How many rows and columns are there?\n",
    "\n",
    " - Which columns are numerical vs categorical?\n",
    "\n",
    " - What does the target variable (income) look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code solution here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412d3ec",
   "metadata": {},
   "source": [
    "***Part 2: Investigate Data Quality***\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Check for missing values with `.isna().sum().`\n",
    "\n",
    "2. Explore unique values in categorical columns.\n",
    "\n",
    "3. Look for \"?\" or special characters in categorical columns.\n",
    "\n",
    "4. Check for duplicate rows with `.duplicated().sum().`\n",
    "\n",
    "Questions\n",
    "\n",
    " - Which columns contain missing values (or \"?\")?\n",
    "\n",
    " - Are there strange category values (e.g., extra spaces, symbols)?\n",
    "\n",
    " - How many duplicate rows exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b040fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba82e3",
   "metadata": {},
   "source": [
    "***Part 3: Handle Missing Values***\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Replace \"?\" with NaN.\n",
    "\n",
    "2. Decide: drop rows/columns or fill missing values.\n",
    "\n",
    "3. Justify your choice.\n",
    "\n",
    "Questions\n",
    "\n",
    " - What percentage of values are missing per column?\n",
    "\n",
    " - For columns with missing data, should you drop or fill? Why?\n",
    "\n",
    " - If filling, which method makes sense (mode, median, ‚ÄúUnknown‚Äù)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92752cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show your code solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c19bb2",
   "metadata": {},
   "source": [
    "***Part 4: Clean Categorical Data***\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Remove leading/trailing spaces with `.str.strip()`.\n",
    "\n",
    "2. Standardize text case (e.g., lowercase all values).\n",
    "\n",
    "3. Fix inconsistencies (e.g., ‚Äúself-emp-not-inc‚Äù vs ‚Äúself-emp-inc‚Äù).\n",
    "\n",
    "Questions\n",
    "\n",
    " - Which columns had inconsistent or messy categories?\n",
    "\n",
    " - After cleaning, how many unique categories remain in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddcc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for handle Missing Values\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9026d07",
   "metadata": {},
   "source": [
    "***Part 5: Handle Duplicates***\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Count duplicates.\n",
    "\n",
    "2. Drop them if necessary.\n",
    "\n",
    "Questions\n",
    "\n",
    " - How many duplicates were found?\n",
    "\n",
    " - Does dropping them reduce dataset size significantly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80fa7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for handle duplicates\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d34afc",
   "metadata": {},
   "source": [
    "***Part 6: Feature Selection***\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Discuss which columns may not be useful for predicting income (e.g., fnlwgt).\n",
    "\n",
    "2. Drop irrelevant columns.\n",
    "\n",
    "Questions (Write your answers in markdown cells)\n",
    "\n",
    "- What does fnlwgt represent? Should it be used?\n",
    "\n",
    "- Which features are most likely to influence income?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0f3b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ef139",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../Images/datasafari-logo-primary.png\" width=\"300\">\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
